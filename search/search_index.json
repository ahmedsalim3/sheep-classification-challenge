{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sheep Classification Challenge","text":"<p>A deep learning solution for the Kaggle Sheep Classification Challenge 2025, achieving 0.97046 F1-score using semi-supervised learning techniques on a small, imbalanced dataset.</p>"},{"location":"#challenge-overview","title":"Challenge Overview","text":"<p>The goal was to classify 7 sheep breeds using just 682 labeled images with significant class imbalance and visually noisy data. The evaluation metric was F1-score, making this a particularly challenging task.</p> <p>Key Challenges: - Extremely small dataset (682 images) - High class imbalance across 7 breeds - Visually noisy images with poor quality - F1-score evaluation requiring balanced precision/recall</p>"},{"location":"#solution-approach","title":"Solution Approach","text":"<p>Our solution employs a semi-supervised learning pipeline built around Vision Transformers (ViT) with intelligent data mining techniques:</p>"},{"location":"#1-initial-training","title":"1. Initial Training","text":"<ul> <li>5-fold cross-validation on clean labeled data</li> <li>Vision Transformer (ViT) architecture</li> <li>Focal Loss + Class Weights for imbalance handling</li> <li>Custom training loop with dynamic learning rate scheduling</li> </ul>"},{"location":"#2-pseudo-labeling","title":"2. Pseudo-labeling","text":"<ul> <li>Ensemble predictions on unlabeled test set</li> <li>Strict confidence threshold (\u2265 0.97) for quality control</li> <li>Automatic filtering of low-confidence predictions</li> </ul>"},{"location":"#3-clustering-based-data-mining","title":"3. Clustering-based Data Mining","text":"<ul> <li>K-Means clustering on ViT feature embeddings</li> <li>Purity threshold (\u2265 0.9) for cluster filtering</li> <li>Extracted 34 high-quality samples from unlabeled data</li> <li>Feature space similarity for automatic labeling</li> </ul>"},{"location":"#4-final-training","title":"4. Final Training","text":"<ul> <li>Combined dataset: clean + pseudo-labeled + clustered samples</li> <li>~115 total high confidence samples (78% exposed of test unlabeled set)</li> <li>Ensemble of 10 models (5 initial + 5 final)</li> <li>Weighted ensemble using cross-validation scores</li> </ul>"},{"location":"#results-performance","title":"Results &amp; Performance","text":"Metric Value Best Kaggle F1-Score 0.97046 Dataset Expansion 682 + ~115 synthetic samples \u2192 ~797 total Unlabeled Data Utilization 78% Clustered Samples Extracted 34 samples Model Ensemble Size 10 models"},{"location":"#technical-implementation","title":"Technical Implementation","text":""},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Base Model: Vision Transformer (ViT) via <code>timm</code> library, <code>vit_base_patch16_224.augreg_in21k_ft_in1k</code> variant</li> <li>Loss Function: Focal Loss with effective sample weighting</li> <li>Optimizer: AdamW with dynamic learning rate scheduling</li> <li>Data Augmentation: Albumentations for robust training</li> </ul>"},{"location":"#key-innovations","title":"Key Innovations","text":"<ul> <li>Confidence-based filtering prevents pseudo-label noise</li> <li>Clustering purity checks ensure high-quality synthetic samples</li> <li>Weighted ensemble balances clean vs. pseudo-labeled models</li> <li>Effective class weighting handles severe imbalance</li> </ul>"},{"location":"#key-insights-learnings","title":"Key Insights &amp; Learnings","text":""},{"location":"#what-worked","title":"What Worked","text":"<ul> <li>High confidence thresholds (\u22650.97) for pseudo-labeling</li> <li>Clustering with purity checks extracted valuable samples</li> <li>Ensemble diversity through different training strategies</li> <li>Focal Loss + Class Weights handled imbalance effectively</li> </ul>"},{"location":"#what-didnt-work","title":"What Didn't Work","text":"<ul> <li>Lower confidence thresholds introduced noise</li> <li>Blind pseudo-labeling without filtering</li> <li>Single model approaches</li> <li>Standard cross-entropy loss</li> </ul>"},{"location":"#best-practices-discovered","title":"Best Practices Discovered","text":"<ul> <li>Quality over quantity in synthetic data generation</li> <li>Consistent feature space for clustering effectiveness</li> <li>Balanced ensemble weighting for optimal performance</li> <li>Robust data augmentation for small datasets</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>GitHub Repository</li> <li>Kaggle Notebook</li> <li> <p>Competition Page</p> </li> <li> <p>Albumentations</p> </li> <li>An Image is Worth 16x16 Words</li> <li>timm-model: vit_base_patch16_224.augreg_in21k_ft_in1k</li> <li>UMAP: Uniform Manifold Approximation and Projection</li> <li>A Density-Based Algorithm for Discovering Clusters in Large Spatial Databaseswith Noise</li> <li>Emerging Properties in Self-Supervised Vision Transformers</li> <li>Self-Supervised Representation Learning</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is open source and available under the MIT License.</p> <p>Built with \u2764\ufe0f for the Kaggle Sheep Classification Challenge 2025</p>"}]}